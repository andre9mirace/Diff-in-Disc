---
title: "BW_Functions"
output:
  pdf_document: default
  html_document: default
date: "2024-04-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Ce document recapitule l'ensemble de fonctions utilisées pour trouver la bandwidth optimale pour estimation d'un effet de traitement avec la méthode de Difference-in-Regression-Discontinuity (DiRD) ou Diff-in-Disc.

# Imbens Plug-in Bandwidth Function
```{r}
Imbens_BW <- function(y, x, c, kernel="triangular"){
  
  df <- data.frame(y=y, x=x)
  df <- na.omit(df)
  N <- nrow(df)
  
  # Uniform kernel
  if (kernel == "uniform") {
   C_k <- 5.40
  } 
    # Triangular kernel as default
  else {
    C_k <- 3.4375
  }

  # Prepare the df for regression
  df$group_indc <- ifelse(df$x>c, 1, 0) # group indicator for regression
  df$x1 <- (df$x-c)
  df$x2 <- (df$x-c)^2
  df$x3 <- (df$x-c)^3
  
  # Subset into left/right of the cutoff
  sample_plus <- subset(df, x > c)
  sample_minus <- subset(df, x < c)
  N_plus <- nrow(sample_plus)
  N_minus <- nrow(sample_minus)
  
  # First step
  # We first estimate the running variable's density and the limit of the conditional variances at the cutoff 
  
  h1 <- 1.84*sd(x)*N^(-1/5) # Standard Silverman rule of thumb
  silver_sample_plus <- subset(sample_plus, x < c+h1)
  silver_sample_minus <- subset(sample_minus, x > c-h1)
  
  running_density_c <- (nrow(silver_sample_minus)+nrow(silver_sample_plus))/(2*N*h1) # Density of X at the cutoff
  
  # Conditional variance limits (c+ and c-)
  cond_var_c_plus <- var(silver_sample_plus$y)
  cond_var_c_minus <- var(silver_sample_minus$y)
  
  # Second step
  # Estimation of two bandwidths to estimate the second order derivative.

  # Quadratic regression
  reg_m3 <- lm(y ~ group_indc + x1 + x2 + x3, data = df) # maybe adding time_indc for DiRD?
  
  m3_c <- 6*coef(reg_m3)[[5]] # estimate of the 3rd derivative to the regression function 

  
  h2_plus <- 3.56*((cond_var_c_plus)/(running_density_c*(m3_c)^2*N_plus))^(1/7)
  h2_minus <- 3.56*((cond_var_c_minus)/(running_density_c*(m3_c)^2*N_minus))^(1/7)
  
  sample_2_plus <- subset(sample_plus, sample_plus$x < c+h2_plus)
  sample_2_minus <- subset(sample_minus, sample_minus$x > c-h2_minus)
  
  
  # Quadratic regression to estimate 
  quad_reg_plus <- lm(y ~ x1 + x2, data = sample_2_plus)
  quad_reg_minus <- lm(y ~ x1 + x2, data = sample_2_minus)
  
  m2_plus <- 2*coef(quad_reg_plus)[[3]]
  m2_minus <- 2*coef(quad_reg_minus)[[3]]

  # Third step
  r_plus <- 2160*cond_var_c_plus/(nrow(sample_2_plus)*h2_plus^4)
  r_minus <- 2160*cond_var_c_minus/(nrow(sample_2_minus)*h2_minus^4)
  
  h_optimal <- C_k*N^(-1/5)*((cond_var_c_plus+cond_var_c_minus)
                             /(running_density_c*(m2_plus-m2_minus)^2+r_plus+r_minus))^(1/5)
  return(h_optimal)
}
```


# Prior Functions

## rdd_k_weights
Calculates kernel weights for observations in a data set given a specific kernel type. These weights are used in regression discontinuity design (RDD) analyses. Very similar function to Cattaneo's rdrobust "rdrobust_kweight" function.

```{r}
rdd_k_weights <- function(x, c, h, kernel="triangular"){
  # Normalize the distances from the cutoff by dividing by the bandwidth (h).
  u = (x - c) / h
  
	# Uniform kernel
  if (kernel == "uniform") {
   w = (0.5 * (abs(u) <= 1)) / h
  } 
    # Triangular kernel as default
  else {
     w = ((1 - abs(u)) * (abs(u) <= 1)) / h
  }
  return(w)  
}
```

## MSE_DiRD
Cette fonction nous permet de calculer la MSE d'une régression linéaire adaptée a un contexte de Diff-in-Disc. Cette équation d'estimation correspond à celle qui a été présentée dans l'article de V. Grembi 'Do Fiscal Rules Matter?' (2016) :
$$ y_{it} = \beta_0+\beta_1X_i+ 1_{\{X\geq c\}}(\alpha_0+\alpha_1X_i)+1_{\{t\geq t_0\}}(\gamma_0+\gamma_1X_i)+
1_{\{X\geq c\}}1_{\{t\geq t_0\}}(\tau+\lambda_0X_i)+\varepsilon_{it} $$
Où $\tau$ est l'effet du traitement.

```{r}
MSE_DiRD <- function(y, x, time_var, c, t0, h, kernel = "triangular"){
  # Calculate weights
  w <- rdd_k_weights(x=x, c=c, h=h, kernel = kernel)
  
  # Prepare regression
  reg_data <- data.frame(y, x, time_var)
  
  # Treatment start indicator
  reg_data$time_indc <- ifelse(reg_data$time_var >= t0, 1, 0)
  
  # Treatment group indicator
  reg_data$group_indc <- ifelse(reg_data$x >= c, 1, 0)
  
  # Running Variable
  # reg_data$x1 <- reg_data$x-c
  
  # Interaction between time and treated
  reg_data$dird <- reg_data$time_indc * reg_data$group_indc
  
  # Check if the data is valid for regression
  if (nrow(reg_data) == 0 | sum(!is.na(reg_data$y)) == 0) {
    print("Insufficient data")
  }
    else{
      # Regression model
      model<- lm(y ~ x + group_indc*(1+x) + time_indc*(1+x) + dird*(1+x), 
                 data=reg_data, weights = w)
      mse <- mean(resid(model)^2)
    }
  return(mse)
  }
```


```{r}
MSE_penalty <- function(N, n_h, penalty_type="default"){
  
  if (penalty_type == "default") {
    penalty <- (N / n_h)
    
    # Log
  } else if (penalty_type == "log") {
    penalty <- log(N / n_h + 1)
    
    # Sqrt penalty
  } else if (penalty_type == "sqrt") {
    penalty <- sqrt(N / n_h)
    
    # Cube root pen
  } else if (penalty_type == "cbrt") {  # Cube root
    penalty <- 0.01*(N / (n_h))^(1/3)
    
    # Exp penalty
  } else if (penalty_type == "exp" | penalty_type == "exponential") {
    alpha <- 0.01  
    penalty <- exp(alpha * N / n_h) - 1
  } else if (penalty_type == "sigmoid"|"sig") {
    alpha <- 0.05  
    beta <- 10     
    penalty <- 1 / (1 + exp(-alpha * (N / n_h - beta))) # sigmoid function
  } 
  
  return(penalty)
}

```


## find_next_h
In this context, we "manually" compute the gradient of the MSE function to perform a stochastic gradient descent by randomly selecting a bandwidth value $(h_0)$ and then finding the value for h just above it to compute this "infinitesimal calculation".
```{r}
# Function to find the closest h given a current h value from a BW set
find_next_h <- function(current_h, h_values) {
  # Ensure h_values is not empty
  if (length(h_values) == 0) {
    stop("h_values cannot be empty")
  }
  # Subset to find all values greater than the current h
  possible_values <- h_values[h_values > current_h]
  
  # If there are any higher values, return the minimum
  if (length(possible_values) > 0) {
    return(min(possible_values))
  } else {
    
    # Find values less than the current h and return the maximum of these
    lower_values <- h_values[h_values < current_h]
    if (length(lower_values) > 0) {
      return(max(lower_values))
    } else {
      # No higher or lower values available = stop
      stop("No higher or lower values found for the given h")
    }
  }
}

```

# Main Functions
## MSE_Gradient_0 
This function is used to calculate the Gradient of the MSE corresponding to the randomly chosen $h_0$. 
This will later be used as our initialization for the Stochastic Gradient Descent Algorithm for MSE minimization.This function uses both of the previous functions.\\
MSE_Gradient0 function yields a vector with 2 parameters for results : a random BW (h_0) and its respective MSE Gradient (Gradient_0).
```{r}
MSE_Gradient0 <- function(y, x, h, data, ID, c, time_var, t0, 
                          kernel="triangular", penalty_type="default"){
  
  N <- nrow(data)
  
  # get a df with all unique IDs
  data_unique <- data[!duplicated(data$ID),]
  unique_h_values <- data_unique$h
  
  # Randomly pick a random h and the one just above (h_plus)
  random_index <- sample(1:nrow(data_unique), 1)
  h_0 <- data_unique$h[random_index] # random h
  h_plus <- find_next_h(current_h = h_0, h_values = unique_h_values) # h just above
  
  # Subset by |x| < h and compute MSE (for h_0)
  eval_sample <- subset(data, subset = abs(x-c) < h_0)
  # Repeat the process for h_plus
  sample_plus <- subset(data, subset = abs(x-c) < h_plus)
  
  # save nb of observations in each subset
  n_h <- nrow(eval_sample)
  n_h_plus <- nrow(sample_plus)
  
  if (nrow(eval_sample) == 0 | nrow(sample_plus) == 0) {
    # Handle empty data scenario
    # If data is insufficient, re-run the function
    return(MSE_Gradient0(y, x, h, data, ID, c, time_var, t0, kernel))  # Re run the function
  } else {
    # Calculate MSE for both subsets with respective penalties
    mse_0 <- MSE_penalty(N=N, n_h=n_h, penalty_type=penalty_type)*MSE_DiRD(y = eval_sample$y, x=eval_sample$x, time_var = eval_sample$time_var, c=c, t0 = t0, h=h_0, kernel = kernel)
    
    mse_plus <- MSE_penalty(N=N, n_h=n_h_plus, penalty_type=penalty_type)*MSE_DiRD(y = sample_plus$y, x=sample_plus$x, time_var = sample_plus$time_var, c=c, t0 = t0, h=h_plus, kernel = kernel)
  }
  # Gradient
  MSE_gradient <-  (mse_0-mse_plus)/max((h_0-h_plus), 1e-05)
  return(c(h_0, h_plus, MSE_gradient))
}
```

## DiRD_BW 

This function compiles every function created so far to perform a Stochastic Gradient Descent to minimize the MSE of our regression to find the optimal bandwidth. \\
This function is basically divided in 2 loops : \\
We will call the while loop a for loop that ahs several stopping criteria. The 'while' loop uses a gradient descent algorithm with an ADAGrad method for computing and updating the learning rate.\\ 
The 'for' loop acts as the "stochastic part" in SGD to compare local minima to find a global minimum.\\


```{r}
DiRD_BW <- function(y, x, c, time_var, t0, ID, 
                    kernel = "triangular", penalty_type = "default",
                    max_iter=10, max_epochs=500){
  ## Initialization
  
  # Preparation
  data <- data.frame(ID=ID, y=y, x=x, time_var=time_var)
  data$h <- abs(data$x)
  N <- nrow(data)
  # Initialize parameters : 'for' loop
  h_vector <- c()
  mse_vector <- c()
  initial_lr <- 1
  epsilon <- 1e-8
  
  # h intervals based on quantiles of h
  quantile_division <- c()
  if(N >= 4000){
    quantile_division <- c(0, 0.05, 0.15, 0.25, 0.40, 0.55, 0.75, 1)
  } else if (N < 300) {
    quantile_division <- c(0, 0.45, 1)
  } else {
      quantile_division <- seq(from = 0, to = 1, by = 0.1)
    }
  h_quantiles <- quantile(data$h, probs = quantile_division)
  
  # Loop through each quantile interval
  for (q in 1:(length(h_quantiles) - 1)) {
    h_min <- h_quantiles[q]
    h_max <- h_quantiles[q + 1]
    
    # Subset data within the current h quantile interval
    data <- subset(data, h >= h_min & h < h_max)
    if (nrow(data) == 0) next  # Skip if no data in the current interval
    
    N_int <- nrow(data)
    
    # Get h values in the current interval
    current_h_values <- sort(unique(data$h))
    
    # Initialize empty mse_j
    mse_j <- NA
    mse_j_plus <- NA
    
    for(i in 1:max_epochs){
      
      # Initialization : h0 and gradient0 to do a stochastic gradient descent
      gradient_details <- MSE_Gradient0(y=y, x=x, h=data$h, data=data, ID=ID, c=c,
                                        time_var=time_var, t0=t0, 
                                        kernel = kernel, penalty_type = penalty_type)
      h_0 <- gradient_details[1]
      h_plus <- gradient_details[2]
      Gradient_0 <- gradient_details[3]
      
      # Prepare for iterations
      h_j <- h_0
      Gradient_j <- Gradient_0
      iter_count <- 0
      accumulated_sq_grad <- Gradient_j^2
      adjusted_lr <- log(N_int)*initial_lr / sqrt(accumulated_sq_grad + epsilon)

      # Dynamic threshold
      all_gradients <- c()
      unchanged_iter_count <- 0  # Counter for unchanged h_j

      for(iter in (1:max_iter)){
        iter_count <- iter_count+1
        
        # AdaGrad update for Gradient_j
        accumulated_sq_grad <- accumulated_sq_grad + Gradient_j^2
        adjusted_lr <- log(N_int)*adjusted_lr / sqrt(accumulated_sq_grad + epsilon)
        
        # SGD Update
        h_j_new <- h_j - adjusted_lr * Gradient_j
        
        # Sort data by unique h values to find the closest h value (h_j_plus)
        unique_h_values <- data[!duplicated(data$ID),]
        unique_h_values <- sort(unique(data$h))
        
        h_j_plus <- find_next_h(h_j_new, unique_h_values)
        
        # Handle hj max scenario
        if (h_j_plus < h_j_new){
          break
        }
        
        # Subset by |x| < h and compute MSE 
        sample_j <- subset(data, subset = abs(x-c) < h_j)
        sample_j_new <- subset(data, subset = abs(x-c) < h_j_new)
        sample_j_plus <- subset(data, subset = abs(x-c) < h_j_plus)
       
        # save nb of observations used in the regression
        n_h <- nrow(sample_j)
        n_h_new <- nrow(sample_j_new)
        n_h_plus <- nrow(sample_j_plus)
        
        # Handle empty data scenario
        if (nrow(sample_j_new) == 0 | nrow(sample_j_plus) == 0) {
          break
        } else {
          
          # Compute penalty
          pen_j <- MSE_penalty(N=N, n_h=n_h, penalty_type=penalty_type)
          pen_j_new <- MSE_penalty(N=N, n_h=n_h_new, penalty_type=penalty_type)
          pen_j_plus <- MSE_penalty(N=N, n_h=n_h_plus, penalty_type=penalty_type)
          
          # Calculate penalized MSE for both subsets with a penalty
          mse_j <- pen_j*MSE_DiRD(y = sample_j$y, x=sample_j$x, 
                                  time_var = sample_j$time_var, 
                                  c=c, t0 = t0, h=h_j, kernel=kernel)
          mse_j_new <- pen_j_new*MSE_DiRD(y = sample_j_new$y, x=sample_j_new$x, 
                                          time_var = sample_j_new$time_var, c=c, 
                                          t0 = t0, h=h_j_new, kernel=kernel)
          mse_j_plus <- pen_j_plus*MSE_DiRD(y = sample_j_plus$y, x=sample_j_plus$x,
                                            time_var = sample_j_plus$time_var, c=c, 
                                            t0 = t0, h=h_j_plus, kernel=kernel)
  
          # Compute Gradient
          Gradient_j <- (mse_j_new - mse_j_plus) / (h_j_new - h_j_plus)
        }
        
        
        # Threshold 
        if (abs(mse_j_new-mse_j) < 1/log(N)){
          break
        }
        
        # Check if h_j remains unchanged
        if (h_j == h_j_new) {
          unchanged_iter_count <- unchanged_iter_count + 1
        } else {
          unchanged_iter_count <- 0  # Reset counter if h_j changes
        }
        
        if (unchanged_iter_count >= 2) {
          break  # Break if h_j doesn't change for 4 iterations
        }
        
        h_j <- h_j_new  # Update h_j to the new value
        mse_j <- mse_j_new
      }
      # store local minimizer and minimums for MSE
      h_vector <- c(h_vector, h_j)
      mse_vector <- c(mse_vector, mse_j)
      
    }
  }
  local_minima <- data.frame(h=h_vector, mse=mse_vector)
  local_minima <- na.omit(local_minima)
  
  minimal_mse_h <- local_minima$h[which.min(local_minima$mse)]
  
  return(list(optimal_h = minimal_mse_h, optimal_mse = min(local_minima$mse)))
}
```

